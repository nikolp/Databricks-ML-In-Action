# Chapter 4: Getting to Know Your Data

**Here is what you will learn as part of this chapter:**

1. Improving data integrity with Delta Live Tables (DLT)
2. Monitoring data quality with Databricks Lakehouse Monitoring
3. Exploring data with Databricks Assistant
4. Generating data profiles with AutoML
5. Using embeddings for machine-readable data
6. Enhancing data retrieval with Databricks Vector Search
7. Applying our learning

## Technical requirements 

Here are the technical requirements needed to complete the hands-on examples in this chapter:
- The [Databricks Assistant](https://docs.databricks.com/en/notebooks/notebook-assistant-faq.html) is a newer feature that an administrator can enable. We will show the Assistant in this chapter.
- We use the [missingno](https://pypi.org/project/missingno/) library to address missing numbers in our project data. 
- In the section using AutoML, we reference the [AutoML-generated notebook](https://github.com/PacktPublishing/Databricks-Lakehouse-ML-In-Action/blob/0dbe21cdd3e11ff9295048d4d07bec14d037150e/Chapter_4_Cleaning_and_exploring/Favorita%20Forecasting%20Exploration/Autogenerated%20Data%20Exploration%20Notebook.py), which you can find in the GitHub repository. 
  
## Links

**In the chapter**
- [ydata-profiling](https://ydata-profiling.ydata.ai/docs/master/index.html)

**Further Reading**
- [Enabling visualizations with Aggregations in DBSQL](https://docs.databricks.com/sql/user/visualizations/index.html#enable-aggregation-in-a-visualization)
- [Using the ydata profiler to explore data](https://ydata-profiling.ydata.ai/docs/master/index.html)
- [Advancing Spark - Meet the new Databricks Assistant](https://youtu.be/Tv8D72oI0xM)
- [Introducing the new Databricks Assistant](https://www.databricks.com/blog/introducing-databricks-assistant)
